{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-thunderseg","title":"Welcome to Thunderseg !","text":""},{"location":"#testtest","title":"testtest","text":"<p>Thunderseg is an open-source package built on top of pytorch and pytorch-lightning. This package focus on deep learning image segmentation tasks for remote sensing applications, including vegetation, cropland, artificial structures, etc. </p> <p>The goal of this package is to collect the state-of-art image segmentation methods and provide painless image segmentation workflow to the users.  </p> <p>This document will assume you have fundimental understanding of: </p> <ul> <li>Python </li> <li>Linux CLI</li> <li>Conda</li> <li>Docker (Optional)</li> <li>Apptainer (Optional)</li> </ul> <p>You may click these hyperlinks above to access corresponding tutorial.</p>"},{"location":"#system-requirement","title":"System Requirement","text":"<p>Thunderseg should be able to run on any Linux-based system.  The package has been tested in Ubuntu 22.04.4 LTS running under WSL2. </p> <p>Test environment:</p> Category Model CPU AMD Ryzen 7 7840HS GPU Nvidia 4070 Super Memory 64GB DDR5 Operate System Windows 10 Pro N 64-bit WSL2"},{"location":"#need-help","title":"Need Help?","text":"<p>Meanwhile, if you have any question relate to the package or the documentation, please feel free to submit ,  join ,  or send me </p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about","title":"About","text":""},{"location":"flowchart/main_program/","title":"Main Program","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> <p>h</p>"},{"location":"getting_start/","title":"Installation","text":""},{"location":"getting_start/#test","title":"Test","text":"<p>Setting up a software environment is a painful experience. Thunderseg provides several methods to get the environment ready.</p> <p>The installation process will take roughly 5-10 minutes and may vary depends on the network condition and computer resource.</p>"},{"location":"getting_start/#via-conda","title":"Via Conda","text":"Don't know how to install Conda? <p> Check Miniconda install guide</p> <p>OR </p>   Simply do    <pre><code>  mkdir -p ~/miniconda3\n  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n  rm ~/miniconda3/miniconda.sh\n</code></pre> <pre><code>conda env create -f https://raw.githubusercontent.com/jldz9/thunderseg/refs/heads/master/environment.yml\npip install thunderseg\n</code></pre>"},{"location":"getting_start/#via-container","title":"Via Container","text":"Don't know how to install Docker? <p> Check Docker install guide</p> Warning <p>If you want to activate GPU processing while using docker, using NVIDIA GPUs with CUDA support is necessary. </p> <p>Additionally, you will need to install NVIDIA Container Toolkit </p>"},{"location":"getting_start/#use-dockerfile","title":"&gt; Use dockerfile","text":"<p><pre><code>wget https://raw.githubusercontent.com/jldz9/thunderseg/refs/heads/master/.devcontainer/Dockerfile \ndocker build -t thunderseg:1.0.0.dev25 .\n</code></pre> After running above code block, you should be able to find image in docker desktop image tab    or in command line:  <pre><code>user:~$ docker images\nREPOSITORY   TAG       IMAGE ID       CREATED             SIZE\nthunderseg   1.0.0.dev25     ba968b128eda   About an hour ago   20.5GB\n</code></pre></p>"},{"location":"getting_start/#use-image-from-docker-hub","title":"&gt; Use image from Docker Hub","text":"<pre><code>docker pull jldz9/thunderseg:1.0.0.dev25\n</code></pre>"},{"location":"getting_start/#use-apptainer","title":"&gt; Use apptainer","text":"Don't know how to install Apptainer? <p> Check Apptainer install guide</p> <p>Apptainer is a docker alternative in order to run images on HPC environment, normally you don't need to install it locally.</p> <p>You can load apptainer by using <code>module load apptainer</code> under HPC environment that runs SLURM workload manager</p> <p>To pull image from docker hub using apptainer: </p> <pre><code>apptainer pull thunderseg_100dev.sif docker://jldz9/thunderseg:1.0.0.dev25\n</code></pre>"},{"location":"getting_start/#via-source-code","title":"Via Source code","text":"<p>If you would like to contribute to this project, thanks in advance! </p> <p>You can pull the source code from GitHub by: <pre><code>git clone https://github.com/jldz9/thunderseg.git\n</code></pre> and then set up the environment by:  <pre><code>conda env create -f thunderseg/environment.yml\n</code></pre> we also provide devcontainer.json for vscode under .devcontainer directory</p>"},{"location":"getting_start/basic_usage/","title":"Basic Usage","text":"<p>Thunderseg offers both CLI entry point and Python API for production and research, users who are familiar with Python could build their own functions and modules on top of Thunderseg.</p>"},{"location":"getting_start/basic_usage/#cli-usage","title":"CLI usage","text":"<p>If Thunderseg is properly installed, you should be able to run command <code>thunderseg</code> in CLI, try:</p> <pre><code>thunderseg --help \n</code></pre> <p>In order to run this tutorial, you can download example dataset to your home directory by using:  <pre><code>thunderseg -e \n</code></pre> After download completed, a <code>thunderseg_example_data</code> folder should exist in your home directory with structure: </p> <p><pre><code>thunderseg_example_data\n\u251c\u2500\u2500 predict  #Place to put your prediction raster\n|   \u2514\u2500\u2500 Drake20220819_MS.tif\n\u251c\u2500\u2500 shp\n\u2502   \u251c\u2500\u2500 train_shp # Place to put your training shape file \n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.cpg\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.dbf\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.prj\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.shp\n\u2502   \u2502   \u2514\u2500\u2500 shp_20220928.shx\n\u2502   \u251c\u2500\u2500 valid_shp # Place to put your validation shape file \n\u2502   \u2502   \u251c\u2500\u2500 valid.cpg\n\u2502   \u2502   \u251c\u2500\u2500 valid.dbf\n\u2502   \u2502   \u251c\u2500\u2500 valid.prj\n\u2502   \u2502   \u251c\u2500\u2500 valid.shp\n\u2502   \u2502   \u2514\u2500\u2500 valid.shx\n\u2502   \u2514\u2500\u2500 test_shp \n\u2514\u2500\u2500 train # Place to put your training raster\n    \u2514\u2500\u2500 Drake20220928_MS.tif\n</code></pre> This is the default directory structure to prepare for Thunderseg program.  Each folder inside workdir directory is explained below: </p> Name Note necessity <code>predict</code> Place to put your raster for predict process, can be multiple raster optional <code>shp/train_shp</code> Place to put your Shapefile for training purpose, can be multiple Shapefile required <code>shp/valid_shp</code> Place to put your Shapefile for validate purpose, can be multiple Shapefile if empty will split train Shapefile for valid process <code>shp/test_shp</code> Place to put your Shapefile for test purpose, can be multiple Shapefile if empty will split valid Shapefile for test process <code>train</code> Place to put your raster for train process, can be multiple raster required"},{"location":"getting_start/basic_usage/#step-1-copy-and-setup-config","title":"&gt; Step 1: Copy and Setup Config","text":"<p>Thunderseg CLI entry point uses configure file through the entire process.  Run below command to get default configuration file copied to <code>thunderseg_example_data</code> folder. <pre><code>thunderseg -g ~/thunderseg_example_data\n</code></pre></p> <p>You should find a <code>config.toml</code> file under <code>thunderseg_example_data</code> folder looks like below: </p> Example <pre><code># This is the config file for thunderseg program. Program use metric system\n[IO]\nWORKDIR = \"/\" # Your Path to the working directory\nTRAIN_RASTER_DIR = \"/\" # Your Path to the training raster directory\nTRAIN_SHP_DIR = \"/\" # Your Path to the training Shapefile directory\nVALID_SHP_DIR = \"/\" # Your Path to the validation Shapefile directory\nPREDICT_RASTER_DIR = \"/\" # Your Path to the prediction raster directory\n\n[PREPROCESS]\nTILE_SIZE = 100 # meter\nBUFFER_SIZE = 10 # meter\nDEBUG = false\nMODE = \"BGR\"\n\n[PREPROCESS.RESAMPLE]\nENABLE = false\nRESOLUTION = 0.1 # meter\n\n[PREPROCESS.TRANSFORM]\nRANDOM_CROP_HEIGHT = 512\nRANDOM_CROP_WIDTH = 512\n\n[PREPROCESS.COCO_INFO]\nNAME = \"testdataset\"\nVERSION = \"1.0\"\nDESCRIPTION = \"This is a test dataset\"\nCONTRIBUTOR = \"unknown\"\nURL = \"unknown\"\n# You can add as much descriptions as you want under this section.\n# e.g.\n# time = '2019-08-24'\n# email = 'myemail@email.com'\n\n[TRAIN]\nMODEL = \"maskrcnn_rgb\" # other model option in the furture\nMAX_EPOCHS = 100\nNUM_CLASSES = 2\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 5\nNUM_WORKERS = 15\n</code></pre> <p>You need to change PATH in IO section in the config file to corresponding paths in <code>thunderseg_example_data</code> directory: </p> Example <p> Make sure you use your own path instead of copy the exact same path provided below, or you will get file not found error later. You can use <code>pwd</code> to find path of your current work directory. <pre><code># This is the config file for thunderseg program. Program use metric system\n[IO]\nWORKDIR = \"/home/jldz9/thunderseg_example_data/workdir\" # Your Path to the working directory\nTRAIN_RASTER_DIR = \"/home/jldz9/thunderseg_example_data/train\" # Your Path to the training raster directory\nTRAIN_SHP_DIR = \"/home/jldz9/thunderseg_example_data/shp/train_shp\" # Your Path to the training Shapefile directory\nVALID_SHP_DIR = \"/home/jldz9/thunderseg_example_data/shp/valid_shp\" # Your Path to the validation Shapefile directory\nPREDICT_RASTER_DIR = \"/home/jldz9/thunderseg_example_data/predict\" # Your Path to the prediction raster directory\n\n[PREPROCESS]\nTILE_SIZE = 100 # meter\nBUFFER_SIZE = 10 # meter\nDEBUG = false\nMODE = \"BGR\"\n\n[PREPROCESS.RESAMPLE]\nENABLE = false\nRESOLUTION = 0.1 # meter\n\n[PREPROCESS.TRANSFORM]\nRANDOM_CROP_HEIGHT = 512\nRANDOM_CROP_WIDTH = 512\n\n[PREPROCESS.COCO_INFO]\nNAME = \"testdataset\"\nVERSION = \"1.0\"\nDESCRIPTION = \"This is a test dataset\"\nCONTRIBUTOR = \"unknown\"\nURL = \"unknown\"\n# You can add as much descriptions as you want under this section.\n# e.g.\n# time = '2019-08-24'\n# email = 'myemail@email.com'\n\n[TRAIN]\nMODEL = \"maskrcnn_rgb\" # other model option in the furture\nMAX_EPOCHS = 100\nNUM_CLASSES = 2\nLEARNING_RATE = 1e-3\nBATCH_SIZE = 5\nNUM_WORKERS = 15\n</code></pre></p>"},{"location":"getting_start/basic_usage/#step-2-run-preprocess","title":"&gt; Step 2: Run Preprocess","text":"<p>Preprocess module tiles input raster into same size tiles and then convert to a COCO dataset.</p> <p>Run code below to start preprocess:  <pre><code>thunderseg preprocess -c ~/thunderseg_example_data/config.toml\n</code></pre> A <code>workdir</code> will be generated under <code>thunderseg_example_data</code>, the directory structure will look like below:</p> Example <pre><code>\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 predict\n\u251c\u2500\u2500 shp\n\u2502   \u251c\u2500\u2500 train_shp\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.cpg\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.dbf\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.prj\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.qix\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.qmd\n\u2502   \u2502   \u251c\u2500\u2500 shp_20220928.shp\n\u2502   \u2502   \u2514\u2500\u2500 shp_20220928.shx\n\u2502   \u2514\u2500\u2500 valid_shp\n\u2502       \u251c\u2500\u2500 valid.cpg\n\u2502       \u251c\u2500\u2500 valid.dbf\n\u2502       \u251c\u2500\u2500 valid.prj\n\u2502       \u251c\u2500\u2500 valid.shp\n\u2502       \u2514\u2500\u2500 valid.shx\n\u251c\u2500\u2500 train\n\u2502   \u2514\u2500\u2500 Drake20220928_MS.tif\n\u2514\u2500\u2500 workdir\n    \u251c\u2500\u2500 datasets\n    \u2502   \u251c\u2500\u2500 annotations\n    \u2502   \u2502   \u251c\u2500\u2500 train_coco.json\n    \u2502   \u2502   \u2514\u2500\u2500 valid_coco.json\n    \u2502   \u251c\u2500\u2500 predict\n    \u2502   \u251c\u2500\u2500 train\n    \u2502   \u2502   \u251c\u2500\u2500 Drake20220928_MS_row0_col0.png\n    \u2502   \u2502   \u251c\u2500\u2500 Drake20220928_MS_row0_col0.tif\n    \u2502   \u2502   \u251c\u2500\u2500 Drake20220928_MS_row0_col11151.png\n    \u2502   \u2502   \u251c\u2500\u2500 Drake20220928_MS_row0_col11151.tif\n    \u2502   \u2502   \u251c\u2500\u2500 Drake20220928_MS_row0_col1239.png\n    \u2502   \u2502   \u251c\u2500\u2500 Drake20220928_MS_row0_col1239.tif\n    \u2502   \u2502   \u251c\u2500\u2500 .......\n    \u2502   \u2502   \u2514\u2500\u2500 shp\n    \u2502   \u2502       \u251c\u2500\u2500 train_shp.cpg\n    \u2502   \u2502       \u251c\u2500\u2500 train_shp.dbf\n    \u2502   \u2502       \u251c\u2500\u2500 train_shp.prj\n    \u2502   \u2502       \u251c\u2500\u2500 train_shp.shp\n    \u2502   \u2502       \u2514\u2500\u2500 train_shp.shx\n    \u2502   \u2514\u2500\u2500 val\n    \u2502       \u2514\u2500\u2500 shp\n    |           \u251c\u2500\u2500 valid_shp.cpg\n    \u2502           \u251c\u2500\u2500 valid_shp.dbf\n    \u2502           \u251c\u2500\u2500 valid_shp.prj\n    \u2502           \u251c\u2500\u2500 valid_shp.shp\n    \u2502           \u2514\u2500\u2500 valid_shp.shx\n    \u251c\u2500\u2500 results\n    \u2514\u2500\u2500 temp\n        \u251c\u2500\u2500 Drake20220928_MS_coco.json\n        \u2514\u2500\u2500 Drake20220928_MS_coco_valid.json\n</code></pre> <p>Each folder under workdir directory is explained below: </p> Name Note <code>dataset</code> Storage all output data during the process <code>dataset/annotation</code> Stitched COCO dataset based on user provide Shapefile <code>dataset/predict</code> Place to storage predict tiles during predict stage <code>dataset/train</code> Place to storage train tiles during preprocess and train stage <code>dataset/train/shp</code> Stitched training Shapefile <code>dataset/val/shp</code> stitched validation Shapefile <code>results</code> Place to storage all final products and logs <code>temp</code> Place to storage temporary files for debug uses <p><code>*png</code> files generated during processing is used to preview the tiling result. </p>"},{"location":"getting_start/basic_usage/#step-3-run-train","title":"&gt; Step 3: Run Train","text":"<p>Train process use model script as plugins, you can prepare your own model script (  Not available in preview version yet )</p> <p>You can call built in model script by using corresponding model name in the config file under TRAIN section e.g.:  <pre><code>[TRAIN]\nMODEL = \"maskrcnn_rgb\" \n</code></pre> OR you can create your own model script and point MODEL to absolute path of your model script, You can check more info about how to prepare your own model script <pre><code>[TRAIN]\nMODEL = \"/home/jldz9/mymodels/foo.py\" \n</code></pre></p> <p>Currently, Thunderseg has one built in model scripts for instance segmentation</p> Model description maskrcnn_rgb A classic instance segmentation model, use maskrcnn_resnet50_fpn_v2 from torchvision <p>Once select the model script in the config file, run: </p> <p><pre><code>thunderseg train -c ~/thunderseg_example_data/config.toml\n</code></pre> The training process will start, and you should be able to check your training process via Tensorboard at <code>http://localhost:6006/</code> in browser.</p> <p>Once training process is finished, the model state will be saved as a checkpoint file (<code>.ckpt</code>) inside the log folder, you can check pytorch-lightning documentation for more description of checkpoint file. The output directory structure should look like below: </p> Example <pre><code>\u251c\u2500\u2500 results\n\u2502   \u2514\u2500\u2500 logs\n\u2502       \u251c\u2500\u2500 version_0\n\u2502       \u2502   \u251c\u2500\u2500 checkpoints\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 epoch=0-step=22.ckpt\n\u2502       \u2502   \u251c\u2500\u2500 events.out.tfevents.1738012420.DESKTOP-RHDONR9.11097.0\n\u2502       \u2502   \u2514\u2500\u2500 hparams.yaml\n\u2502       \u2514\u2500\u2500 version_1\n\u2502           \u251c\u2500\u2500 checkpoints\n\u2502           \u2502   \u2514\u2500\u2500 epoch=0-step=22.ckpt\n\u2502           \u251c\u2500\u2500 events.out.tfevents.1738012523.DESKTOP-RHDONR9.16628.0\n\u2502           \u2514\u2500\u2500 hparams.yaml\n</code></pre> Name Note <code>logs</code> Place to storage all training logs <code>logs/version_0</code> or <code>logs/version_*</code> Each version represent a training model, if you run the training multiple times there will be multiple versions of the model <code>logs/version_0/checkpoints</code> The place to save checkpoint file, this is your trained model <code>events.out.tfevents*</code> The tensorboard log file to track your logs during training <code>hparams.yaml</code> Storage all hyperparameters used in training process"},{"location":"getting_start/basic_usage/#step-4-run-predict-and-post-processing","title":"&gt; Step 4: Run Predict and Post-processing","text":"<p>Once you have your model trained, run code below to predict and post-process images: </p> <p><pre><code>thunderseg predict -c ~/thunderseg_example_data/config.toml --ckpt ~/thunderseg_example_data/workdir/results/logs/version_1/checkpoints/epoch=0-step=22.ckpt\n</code></pre> <code>--ckpt</code> option is used to load your checkpoint file form training process.  If <code>--ckpt</code> is not provided, Thunderseg will attempt to find the latest check-point file storage under <code>result</code> directory.</p> <p>After the process, you should be able to find two new files generated under results folder as a <code>*gpkg</code> file, which is a geo-package file that used to load into GIS softwares.</p> <pre><code>result\n\u251c\u2500\u2500 Drake20220819_MS_bounding_box.gpkg\n\u251c\u2500\u2500 Drake20220819_MS_polygon.gpkg\n</code></pre> folder Note <code>*_bounding_box.gpkg</code> The bounding box prediction <code>*_polygon.gpkg</code> The mask prediction <p>That's all, you've completed the whole training loop! </p>"},{"location":"getting_start/basic_usage/#api-usage","title":"API usage","text":"<p>The API document is under development</p>"}]}